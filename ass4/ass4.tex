\documentclass{article}

\usepackage{color}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\today}
\chead{Ying Qiao SID:21412301}
\rhead{Stat215B Sp13: Assignment 4}
\lfoot{}
\cfoot{\thepage}
\rfoot{}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage{listings}
\usepackage{amssymb,amsmath}

%% for inline R code: if the inline code is not correctly parsed, you will see a message
\newcommand{\rinline}[1]{SOMETHING WORNG WITH knitr}
%% begin.rcode setup, include=FALSE
% opts_chunk$set(fig.path='figure/latex-', cache.path='cache/latex-')
%% end.rcode


\begin{document}
\section*{Linear Model}

\hspace{12 pt} With the given model (scalar)
\begin{equation}
\begin{split}
&Y_i = X_i\beta + \epsilon_i \\
&X_i = U_i + 2V_i + \delta_i
\end{split}
\end{equation}
We have $i=1,...,n$,
\begin{equation}
\begin{split}
U_i &\sim^{i.i.d.} N(0, 1)\\
V_i &\sim^{i.i.d.} N(0,1) \\
(\epsilon_i, \delta_i) &\sim^{i.i.d} N(\mu, \Sigma)
\end{split}
\end{equation}
\begin{displaymath}
\mu =
  \begin{pmatrix}
  0 \\
  0
  \end{pmatrix}
, \Sigma = 
  \begin{pmatrix}
  \sigma^2 & \rho \\
  \rho         & \sigma^2
  \end{pmatrix}
\end{displaymath}
So, we can see that $X$ is endogenous and $(U,V)$ are instruments. We
would like to use both OLS and IVLS to estimate parameter $\beta$ and
$\sigma^2$.



\section*{Parameter Estimation}
\hspace{12 pt} In Monte-Carlo simulations (with $N=1000$ runs),
we set the true parameter values to be 
\begin{displaymath}
\beta = 3, \sigma^2 = 1, \rho= 3/4
\end{displaymath}
With $n = 100$ vector $(U_i, V_i, \epsilon_i, \delta_i, X_i,
Y_i)$ samples in each run, we can estimate parameters with different
methods.

%%  begin.rcode ass4-0, cache=TRUE,results="markup",message=FALSE,echo=FALSE
%%rm(list=ls())
%%require(MASS)
%%sqrt.m22 <- function(M){
%%  s <- sqrt(det(M))
%%  t <- sqrt(sum(diag(M)) + 2*s)
%%  R <- M / t
%%  diag(R) <- diag(R) + s / t
%%  R
%%}

%%beta = 3
%%sigma2 = 1
%%rho = 3/4
%%N.MC = 1000
%%n = 100
%%beta.hat.ols <- rep(0, N.MC)
%%beta.hat.ivls <- rep(0, N.MC)
%%sigma2.hat.ivls <- rep(0, N.MC)
%%sigma2.hat.zz <- rep(0, N.MC)

%%for (i in 1:N.MC){
%%set.seed(i)
%%seeds <- round(runif(3)*1000)
%%set.seed(seeds[1])
%%U <- rnorm(n, 0, 1)
%%set.seed(seeds[2])
%%V <- rnorm(n, 0, 1)
%%mu.err <- rep(0, 2)
%%Sigma.err <- matrix(c(sigma2,rho,rho,sigma2),2,2)
%%set.seed(seeds[3])
%%err <- mvrnorm(n, mu.err, Sigma.err)
%%eps <- err[ ,1]
%%del <- err[ ,2]
%%X <- U + 2*V + del
%%Y <- X*beta + eps

%%ols <- lm(Y ~ X - 1)
%%beta.hat.ols[i] <- ols$coeff

%%iv <- lm(X ~ U + V - 1)
%%X.hat <- iv$fit
%%ivls <- lm(Y ~ X.hat - 1)
%%beta.hat.ivls[i] <- ivls$coeff

%%SS <- sum((Y - X*ivls$coeff)^2)
%%sigma2.hat.ivls[i] <- SS / (n - 2)

%%tZ <- t(cbind(U,V))
%%ZZ.sqinv <- sqrt.m22(solve(tcrossprod(tZ)))
%%L <- ZZ.sqinv %*% tZ %*% Y
%%M <- ZZ.sqinv %*% tZ %*% X
%%trans <- lm(L ~ M - 1)
%%sigma2.hat.zz[i] <- sum(trans$res^2)
%%}

%%  end.rcode

\subsubsection*{1. Coefficient parameter estimate}
\hspace{12 pt}  We can get estimates $\hat{\beta}_{OLS},
\hat{\beta}_{IVLS}$ using OLS and IVLS. Here, in real implementation,
IVLS is carried out as two-stage LS. 

%%  begin.rcode ass4-beta, cache=TRUE,results="markup",message=FALSE
%%ols <- lm(Y ~ X - 1)
%%beta.hat.ols[i] <- ols$coeff

%%iv <- lm(X ~ U + V - 1)
%%X.hat <- iv$fit
%%ivls <- lm(Y ~ X.hat - 1)
%%beta.hat.ivls[i] <- ivls$coeff
%%  end.rcode

The statistics from the MC simulations are given below in the table,
and histogram in Figure 1.

\begin{center}
\begin{tabular}{l|r|r|}
($\beta=3$)  & $\hat{\beta}_{OLS}$                 & $\hat{\beta}_{IVLS}$ \\ \hline
mean             & \rinline{mean(beta.hat.ols)} & \rinline{mean(beta.hat.ivls)} \\ \hline
SD                 & \rinline{sd(beta.hat.ols)}      & \rinline{sd(beta.hat.ivls)} \\ \hline
RMS               & \rinline{sqrt(mean((beta.hat.ols - beta)^2))} & \rinline{sqrt(mean((beta.hat.ivls - beta)^2))} \\ 
\end{tabular}
\end{center}


We can see that $\hat{\beta}_{IVLS}$ has smaller bias than
$\hat{\beta}_{OLS}$,  thus smaller root mean-squared error
(RMS). However, $\hat{\beta}_{OLS}$ has smaller standard deviation
(SD) than $\hat{\beta}_{IVLS}$, which can also be seen in the
histogram. Since IVLS is equivalent to two-stage OLS, when getting
better unbiased estimate of the coefficient parameter, the SD of the
estimate is aggregated from the two stages of LS, which is expected to
be larger than simple OLS.

%%  begin.rcode ass4-1, cache=TRUE,echo=FALSE,results="markup",dev='pdf',fig.height=6,fig.width=7,fig.align="center",fig.cap="Histogram of OLS and IVLS coefficient parameter estimates"
%%xlim=range(beta.hat.ivls);
%%lwd=2;col="blue"; xlab="beta hat values"; breaks=seq(140,360,10)
%%par(mfrow=c(2,1),cex.main= 1)
%%hist(beta.hat.ols, xlim=xlim, col=col,lwd=lwd,main="",xlab=xlab,ylab="OLS estimator")
%%hist(beta.hat.ivls,xlim=xlim, col=col,lwd=lwd,main="",xlab=xlab,ylab="IVLS estimator")
%%  end.rcode



\subsubsection*{2. Error variance parameter estimate}
\hspace{12 pt} We estimate the error variance $\sigma^2$ in two
ways. First, we plug in $\hat{\beta}_{IVLS}$ into the original model (1)
and use the residuals for error variance parameter estimation. The
second method that we employ use the residuals from the transformed
equation
\begin{equation}
(Z'Z)^{-1/2}Z'Y = (Z'Z)^{-1/2}Z'X\beta + \eta
\end{equation}
where Z is the $n\times 2$ matrix of instruments, $X$ is the $n\times
1$ design matrix and $Y$ is the $n\times 1$ vector of responses.

%%  begin.rcode ass4-sigma, cache=TRUE,results="markup",message=FALSE
%%SS <- sum((Y - X*ivls$coeff)^2)
%%sigma2.hat.ivls[i] <- SS / (n - 2)

%%tZ <- t(cbind(U,V))
%%ZZ.sqinv <- sqrt.m22(solve(tcrossprod(tZ)))
%%L <- ZZ.sqinv %*% tZ %*% Y
%%M <- ZZ.sqinv %*% tZ %*% X
%%trans <- lm(L ~ M - 1)
%%sigma2.hat.zz[i] <- sum(trans$res^2)
%%  end.rcode

The appropriate denominator for each method and their statistics from the MC
simulations are given below in the table, and histogram in Figure 2.

\begin{center}
\begin{tabular}{l|r|r|}
($\sigma^2=1$)  & Plug-in IVLS                 & Transformed OLS \\ \hline
denominator       & $n-2$                           & $1$ \\ \hline
mean                   & \rinline{mean(sigma2.hat.ivls)}  & \rinline{mean(sigma2.hat.zz)} \\ \hline
SD                       & \rinline{sd(sigma2.hat.ivls)}       & \rinline{sd(sigma2.hat.zz)}   \\
\end{tabular}
\end{center}

We can see that $\hat{\sigma^2}_{IVLS}$ has almost the same bias as
$\hat{\sigma^2}_{trans}$,  but much smaller SD. This can be seen from
the histogram. The plug-in IVLS estimator keeps the normal error variance
characteristics of the OLS. However, since the degree of freedom in
the transformed model is only 2 (two instruments), the error variance
is no longer normal, which gives much larger SD.


%%  begin.rcode ass4-2,cache=TRUE,echo=FALSE,results="markup",dev='pdf',fig.height=6,fig.width=7,fig.align="center",fig.cap="Histogram of IVLS and Transformed OLS error variance estimates"
%%lwd=2;col="blue"; xlab="sigma2 hat values"; breaks=seq(140,360,10)
%%par(mfrow=c(2,1),cex.main= 1)
%%hist(sigma2.hat.ivls, col=col,lwd=lwd,main="",xlab=xlab,ylab="IVLS Plug-in estimator")
%%hist(sigma2.hat.zz,   col=col,lwd=lwd,main="",xlab=xlab,ylab="Transformed estimator")
%%  end.rcode


\end{document}
