\documentclass{article}

\usepackage{color}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\today}
\chead{Ying Qiao SID:21412301}
\rhead{Stat215B Sp13: Assignment 5}
\lfoot{}
\cfoot{\thepage}
\rfoot{}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage{listings}
\usepackage{amssymb,amsmath}
\usepackage{bm}
\usepackage{hhline}

%% for inline R code: if the inline code is not correctly parsed, you will see a message
\newcommand{\rinline}[1]{SOMETHING WORNG WITH knitr}
%% begin.rcode setup, include=FALSE
% opts_chunk$set(fig.path='figure/latex-', cache.path='cache/latex-')
%% end.rcode


\begin{document}
\section*{Math stats}
Efron (2010) exercises

\subsubsection*{1.1}
With the given distribution
\begin{displaymath}
\begin{split}
\mu \sim \mathcal{N}(0,A) \rightarrow  &
f(\mu) = \frac{1}{\sqrt{2 \pi A}} \exp\{-\frac{\mu^2}{2A}\}  \\
z|\mu \sim \mathcal{N}(0,1) \rightarrow & 
f(z|\mu) = \frac{1}{\sqrt{2 \pi}} \exp\{-\frac{(z-\mu)^2}{2}\}
\end{split}
\end{displaymath}
So, we have
\begin{displaymath}
\begin{split}
f(\mu)\cdot f(z|\mu) &= \frac{1}{2\pi \sqrt{A}} \exp \{-\frac{1}{2}
(\frac{A+1}{A}\mu^2 - 2\mu z + z^2)\} \\
& = \frac{1}{2\pi \sqrt{A}} \exp \{-\frac{1}{2B}
(\mu^2 - 2B\mu z + Bz^2)\} ;       B=\frac{A}{A+1} \\
& = \frac{1}{2\pi \sqrt{A}} \exp \{-\frac{1}{2} (\frac{\mu -  Bz}{\sqrt{B}})^2\} 
\cdot \exp \{-\frac{1}{2} (1-B)z^2 \} 
\end{split}
\end{displaymath}
Then, the integral
\begin{displaymath}
\begin{split}
f(z) &= \int {f(\mu)f(z|\mu)} d\mu \\
& = \frac{1}{2\pi \sqrt{A}} \exp \{-\frac{1}{2} (1-B)z^2 \} 
\cdot \sqrt{B} \int_{-\infty}^{+\infty}  
\exp \{-\frac{1}{2} (\frac{\mu -  Bz}{\sqrt{B}})^2\} 
d (\frac{\mu -  Bz}{\sqrt{B}}) \\
& = \frac{\sqrt{B}}{\sqrt{2\pi A}} \exp \{-\frac{1}{2} (1-B)z^2 \} \\
& = \frac{1}{\sqrt{2\pi (A+1)}} \exp \{ -\frac{z^2}{2(A+1)} \} \\
\rightarrow  z & \sim \mathcal{N}(0,A+1)
\end{split}
\end{displaymath}
Finally, we get
\begin{displaymath}
\begin{split}
f(\mu |z) &= \frac{f(\mu)\cdot f(z|\mu)}{f(z)} \\
& = \frac{1}{\sqrt{2\pi B}} \exp \{-\frac{1}{2} (\frac{\mu -  Bz}{\sqrt{B}})^2\} \\
\end{split}
\end{displaymath}
that is, 
\begin{displaymath}
\mu |z \sim \mathcal{N}(Bz, B) , B= A/(A+1)
\end{displaymath}


\subsubsection*{1.2}
(i) (1.17) \newline
We have Bayes estimator
\begin{displaymath}
\hat{\bm{\mu}}^{(Bayes)} = B\bm{z}; B = \frac{A}{A+1}
\end{displaymath}
Then, the risk function (total square error loss)
\begin{displaymath}
\begin{split}
R^{(Bayes)}(\bm{\mu}) &= E_{\bm{z|\mu}}\{||\hat{\bm{\mu}} - \bm{\mu}||^2\} \\
& = E_{\bm{z|\mu}}\{||B\bm{z} - B\bm{\mu} + B\bm{\mu} - \bm{\mu}||^2\} \\
& = B^2 E_{\bm{z|\mu}}\{||\bm{z} - \bm{\mu}||^2\} + (B-1)^2E_{\bm{z|\mu}}\{||\bm{\mu}||^2\}
\end{split}
\end{displaymath}
With the vector $\bm{z|\mu} \sim \mathcal{N}_N(\bm{\mu},\bm{I})$, 
\begin{displaymath}
E_{\bm{z|\mu}}\{||\bm{z} - \bm{\mu}||^2\} = ||\bm{I}_{N\times N}||^2 = N
\end{displaymath}
So, we have finally got
\begin{displaymath}
\begin{split}
R^{(Bayes)}(\bm{\mu}) = B^2 N + (1-B)^2 ||\bm{\mu}||^2
\end{split}
\end{displaymath}
(ii) (1.18) \newline
With the vector $\bm{\mu} \sim \mathcal{N}_N(\bm{0},A\bm{I})$, 
\begin{displaymath}
E_{\bm{\mu}} \{||\bm{\mu}||^2\} = ||A\bm{I}_{N\times N}||^2 = A\cdot N
\end{displaymath}
Using (1.17), the overall Bayes risk
\begin{displaymath}
\begin{split}
R^{(Bayes)} & = E_{\bm{\mu}}\{ R^{(Bayes)}(\bm{\mu}) \} \\
& = B^2 N + (1-B)^2 E_{\bm{\mu}} \{||\bm{\mu}||^2\}; B=\frac{A}{A+1} \\
& = \frac{NA^2}{(A+1)^2} + \frac{1}{(A+1)^2} NA \\
& = \frac{NA}{A+1}
\end{split}
\end{displaymath}


\subsubsection*{1.4}
(a) (1.31) \newline
We have (1.30), that is
\begin{displaymath}
E_{\bm{z|\mu}} \{||\hat{\bm{\mu}} - \bm{\mu}||^2\} 
= E_{\bm{z|\mu}} \{||\bm{z} - \hat{\bm{\mu}}||^2\} - N 
+ 2\sum_{i=1}^N E_{\bm{z|\mu}}\{\frac{\partial \hat{\mu}_i}{\partial z_i}\}
\end{displaymath}
Then, with 
\begin{displaymath}
\begin{split}
\hat{\bm{\mu}}^{(JS)} &= (1 - \frac{N-2}{S}) \bm{z} \\
S &=||\bm{z}||^2= \sum_{i=1}^N z_i^2
\end{split}
\end{displaymath}
We get
\begin{displaymath}
\begin{split}
E_{\bm{z|\mu}} \{||\hat{\bm{\mu}}^{(JS)} - \bm{\mu}||^2\} 
& =  E_{\bm{z|\mu}} \{||\frac{N-2}{S} \bm{z}||^2\} - N + 2\sum_{i=1}^N
E_{\bm{z|\mu}} \{(1-\frac{N-2}{S}) + \frac{N-2}{S^2} 2z_i^2 \}  \\
& = E_{\bm{z|\mu}} \{(\frac{N-2}{S})^2 ||\bm{z}||^2\} - N + 
2 E_{\bm{z|\mu}} \{N(1-\frac{N-2}{S}) + \frac{2(N-2)}{S^2} \sum_{i=1}^N z_i^2\} \\
& = E_{\bm{z|\mu}} \{\frac{(N-2)^2}{S} \} - N + 2N -
2 E_{\bm{z|\mu}} \{\frac{N(N-2)}{S} - \frac{2(N-2)}{S} \} \\
& = N - E_{\bm{z|\mu}} \{\frac{(N-2)^2}{S} \}
\end{split}
\end{displaymath}
(b) (1.24) \newline
We have (1.31), that is shown above; and with marginal 
$\bm{z} \sim \mathcal{N}_N(\bm{0},(A+1)\bm{I})$, 
\begin{displaymath}
E_{\bm{z}} \{ \frac{N-2}{S} \} = \frac{1}{A+1}
\end{displaymath}
So we can derive that
\begin{displaymath}
\begin{split}
R^{(JS)} & = E_{\bm{\mu}} \{ R^{(JS)} (\bm{\mu}) \} \\
& = E_{\bm{\mu}} \{ E_{\bm{z|\mu}} \{||\hat{\bm{\mu}}^{(JS)} - \bm{\mu}||^2\} \} \\
& = N - E_{\bm{\mu}} \{ E_{\bm{z|\mu}} \{ \frac{(N-2)^2}{S} \} \} \\
& = N - (N-2) E_{\bm{z}} \{ \frac{N-2}{S} \} \\
& = N - \frac{N-2}{A+1}  \\
& = N \frac{A}{A+1} + \frac{2}{A+1}
\end{split}
\end{displaymath}


%%  begin.rcode ass5-1-15,cache=TRUE,results="markup",message=FALSE,echo=FALSE
%%rm(list=ls())
%%n <- 10
%%mu <- c(-.81, -.39, -.39, -.08, .69, .71, 1.28, 1.32, 1.89, 4.00)
%%A.hat <- sum(mu^2) / (n-2)
%%TSE <- (n*A.hat+2)/(A.hat+1)
%%  end.rcode
\subsubsection*{1.5}
If we assume in Table 1.2, for $i = 1, ..., n$,
$\mu_i \sim \mathcal{N}(0, A)$, and we also have
$\bm{z|\mu} \sim \mathcal{N}_n(\bm{\mu},\bm{I})$, then we can use all the
conclusions from the hierachical Normal-Normal model. \newline
So based on the derivations in the book, the simulated total square
error of the James-Stein estimator should be close to the theoretical 
value, i.e. Bayes risk of JS estimator.
\begin{displaymath}
TSE \sim R^{(JS)} = (nA + 2)/(A+1)
\end{displaymath}
Here, we can estimate
\begin{displaymath}
\hat{A} = \frac{1}{n-2} \sum_{i=1}^{n=10} \mu_i^2
\end{displaymath}
So, we get $\hat{A}=$ \rinline{A.hat} and $TSE=$ \rinline{TSE}, which is
close to the $8.13$ value in the table.


\newpage
\section*{Simulation}
\hspace{12 pt} In each run of the simulations, $N=1000$ runs,
we set the true $\bm{\mu}=(\mu_1,\dots,\mu_n), n=10$ 
values to be equal to those in Efron's Table 1.2, and generate random samples 
$z_i \sim \mathcal{N}(\mu_i,1)$, i.e., we have $N$ copies of
$\bm{z} \sim \mathcal{N}_n(\bm{\mu},\bm{I})$.


The MLE estimates in each run are just
\begin{displaymath}
\hat{\bm{\mu}}^{(MLE)}_i = z_i
\end{displaymath}
and the James-Stein estimators in each run are
\begin{displaymath}
\begin{split}
\hat{\bm{\mu}}^{(JS)}_i = \bar{z} + (1-\frac{(n-3)\sigma_0^2}{S})\cdot (z_i - \bar{z}) \\
\bar{z} = \frac{1}{n} \sum_{i=1}^n z_i, \hspace{6 pt}
\sigma_0^2 = 1, \hspace{6 pt}
S = \sum_{i=1}^n (z_i - \bar{z})^2
\end{split}
\end{displaymath}
As for the mean squared errors and total squared errors, we have
\begin{displaymath}
\begin{split}
MSE_i^{(MLE)} &= \frac{1}{N} \sum_N (\hat{\mu}_{i,runs}^{(MLE)} - \mu_i)^2; \\
TSE^{(MLE)} &= \sum_{i=1}^n MSE_i^{(MLE)}
\end{split}
\end{displaymath}
And,
\begin{displaymath}
\begin{split}
MSE_i^{(JS)} &= \frac{1}{N} \sum_N (\hat{\mu}_{i,runs}^{(JS)} - \mu_i)^2; \\
TSE^{(JS)} &= \sum_{i=1}^n MSE_i^{(JS)}\\
\end{split}
\end{displaymath}

%%  begin.rcode ass5-2,cache=TRUE,results="markup",message=FALSE,echo=FALSE
%%n <- 10
%%N <- 1000
%%mu.hat.MLE <- matrix(0, nrow=n, ncol=N)
%%mu.hat.JS <- matrix(0, nrow=n, ncol=N)
%%for (i in 1:N){
%%set.seed(i*10)
%%z <- rnorm(n, mu, 1)
%%mu.hat.MLE[ ,i] <- z
%%z.bar <- mean(z)
%%S <- sum((z - z.bar)^2)
%%mu.hat.JS[ ,i] <- z.bar + (1 - (n-3)*1/S)*(z - z.bar)
%%}
%%MSE.MLE <- rowMeans((mu.hat.MLE - mu)^2)
%%MSE.JS <- rowMeans((mu.hat.JS - mu)^2)
%%  end.rcode

So, Efron's Table 1.2 is reproduced as below.
\begin{center}
\begin{tabular}{lrrr} \hline
  & $\mu_i$ & $MSE_i^{(MLE)}$ & $MSE_i^{(JS)}$  \\ \hline
1 & \rinline{sprintf("%.2f",mu[1])} & \rinline{sprintf("%.2f",MSE.MLE[1])} & \rinline{sprintf("%.2f",MSE.JS[1])} \\
2 & \rinline{sprintf("%.2f",mu[2])} & \rinline{sprintf("%.2f",MSE.MLE[2])} & \rinline{sprintf("%.2f",MSE.JS[2])} \\
3 & \rinline{sprintf("%.2f",mu[3])} & \rinline{sprintf("%.2f",MSE.MLE[3])} & \rinline{sprintf("%.2f",MSE.JS[3])} \\
4 & \rinline{sprintf("%.2f",mu[4])} & \rinline{sprintf("%.2f",MSE.MLE[4])} & \rinline{sprintf("%.2f",MSE.JS[4])} \\
5 & \rinline{sprintf("%.2f",mu[5])} & \rinline{sprintf("%.2f",MSE.MLE[5])} & \rinline{sprintf("%.2f",MSE.JS[5])} \\
6 & \rinline{sprintf("%.2f",mu[6])} & \rinline{sprintf("%.2f",MSE.MLE[6])} & \rinline{sprintf("%.2f",MSE.JS[6])} \\
7 & \rinline{sprintf("%.2f",mu[7])} & \rinline{sprintf("%.2f",MSE.MLE[7])} & \rinline{sprintf("%.2f",MSE.JS[7])} \\
8 & \rinline{sprintf("%.2f",mu[8])} & \rinline{sprintf("%.2f",MSE.MLE[8])} & \rinline{sprintf("%.2f",MSE.JS[8])} \\
9 & \rinline{sprintf("%.2f",mu[9])} & \rinline{sprintf("%.2f",MSE.MLE[9])} & \rinline{sprintf("%.2f",MSE.JS[9])} \\
10 & \rinline{sprintf("%.2f",mu[10])} & \rinline{sprintf("%.2f",MSE.MLE[10])} & \rinline{sprintf("%.2f",MSE.JS[10])} \\ \hline
TSE &  & \rinline{sprintf("%.2f",sum(MSE.MLE))} & \rinline{sprintf("%.2f",sum(MSE.JS))}
\end{tabular}
\end{center}
For $MSE$'s, we expect to see one decimal place of agreement 
between my table and Efron's Table 1.2. In real simulation, we
get one decimal place of agreement on the MLE estimates, but zero
on the JS estimates. The one decimal place of agreement on the JS
estimates have some deviations due to simulation randomness.




\newpage
\section*{Shrinking radon}
\hspace{12 pt} In this dataset, we are interested in the observed
radon levels from Minnesota basements. Following the data cleaning
guidelines in the assignment description, we get a total of
511 observations across 17 counties in MN, each with at least 10
observations.

%%  begin.rcode ass5-3-0,cache=TRUE,results="markup",message=FALSE,echo=FALSE
%%rm(list=ls())
%%srrs <- read.table("srrs2.dat", sep=",", header=T)
%%base <- srrs[srrs$floor == 0, ]
%%mn <- base[base$state == 'MN', ]
%%obs.ct <- table(mn$county)
%%maj.ct <- obs.ct[obs.ct >= 10]
%%N <- length(maj.ct)
%%radon <- mn[mn$county %in% names(maj.ct), ]

%%n <- 5
%%train <- c()
%%for (i in 1:N)
%%{
%%tmp <- radon[radon$county == names(maj.ct)[i], ]
%%set.seed(i*10)
%%ids <- sample(1:nrow(tmp), n)
%%train <- rbind(train, tmp[ids, ])
%%}
%%test <- radon[!(radon$idnum %in% train$idnum), ]

%%mu <- rep(0, N)
%%names(mu) <- names(maj.ct)
%%for (i in 1:N)
%%{
%%tmp <- test[test$county == names(maj.ct)[i], ]
%%if ( nrow(tmp) == maj.ct[i] - n )
%%  mu[i] <- mean(tmp$activity)
%%}
%%  end.rcode


A snapshot of the data after cleaning is shown below.
%%  begin.rcode ass5-3-11,cache=TRUE,results="markup",message=FALSE
%%nrow(radon)
%%maj.ct
%%  end.rcode
The dataset is then further splited into two sets:
a training set with 5 randomly chosen observations from
each county, and a test set with the other observations.
%%  begin.rcode ass5-3-12,cache=TRUE,results="markup",message=FALSE
%%head(train)
%%dim(test)
%%  end.rcode

The "true" vector of mean radon levels by county, $\bm{\mu}$, are calculated
using the test data variable $activity$. Our goal is to estimate this
population-level parameter $\bm{\mu}$.

%%  begin.rcode ass5-3-est,cache=TRUE,results="markup",message=FALSE,echo=FALSE
%%mu.MLE <- rep(0, N)
%%wc.se <- rep(0, N) # within-county sum of squared residuals
%%names(mu.MLE) <- names(maj.ct)
%%for (i in 1:N)
%%{
%%tmp <- train[train$county == names(maj.ct)[i], ]
%%if ( nrow(tmp) == n ){
%%  mu.MLE[i] <- mean(tmp$activity)
%%  wc.se[i] <- sum((tmp$activity - mu.MLE[i])^2)
%%}
%%}
%%TSE.MLE <- sum((mu.MLE - mu)^2)

%%mu.bar <- mean(mu.MLE)
%%tau2 <- sum(wc.se) / (N*(n-1))
%%SE <- tau2 / n #note
%%S <- sum((mu.MLE - mu.bar)^2)
%%shr <- 1 - (N - 3)*SE/S
%%mu.JS <- rep(0, N)
%%names(mu.JS) <- names(maj.ct)
%%for (i in 1:N)
%%{
%%tmp <- train[train$county == names(maj.ct)[i], ]
%%if ( nrow(tmp) == n ){
%%  mu.JS[i] = mu.bar + shr*(mu.MLE[i] - mu.bar)
%%}
%%}
%%TSE.JS <- sum((mu.JS - mu)^2)
%%  end.rcode
We adopt the standard independent-normals assumption here, with a slightly
different notation due to the multiple observations for each county.


For county $i$, ($i=1,\dots,N, N=17$), observation $j$, ($j=1,\dots,n, n=5$),
\begin{displaymath}
x_{ij} \sim^{i.i.d} \mathcal{N}(\mu_i,\tau^2)
\end{displaymath}
So the MLE estimates are
\begin{displaymath}
\hat{\mu}_i^{(MLE)}= z_i = x_{i.} = \frac{1}{n} \sum_{j=1}^n x_{ij}
\end{displaymath}
and therefore,
\begin{displaymath}
\hat{\bm{\mu}}^{(MLE)}=\bm{z} \sim \mathcal{N}_N(\bm{\mu},\frac{\tau^2}{n}\bm{I})
\end{displaymath}
As for the James-Stein estimates, we use the pooled-variance technique for estimating
$\tau^2$, because the same number of observations in each county aids the 
assumption of common standard error (SE) among MLE estimators.
\begin{displaymath}
\hat{\mu}_i^{(JS)} = \bar{z} + (1-\frac{(N-3)\cdot \hat{\tau}^2/n}{S})\cdot(z_i - \bar{z})
\end{displaymath}
where
\begin{displaymath}
\begin{split}
\bar{z} &= \frac{1}{N} \sum_{i=1}^N z_i \\
\hat{\tau}^2 &= \frac{1}{N(n-1)} \sum_{i=1}^N \sum_{j=1}^n (x_{ij} - z_i)^2\\
S &= \sum_{i=1}^N (z_i - \bar{z})^2
\end{split}
\end{displaymath}
So, the estimates we get are shown in the table below.

\begin{center}
\begin{tabular}{lrrr} \hline
County & $\mu_i$ & $\hat{\mu}_i^{(MLE)}$ & $\hat{\mu}_i^{(JS)}$ \\ \hline
\rinline{names(maj.ct)[1]} & \rinline{sprintf("%.2f",mu[1])} & \rinline{sprintf("%.2f",mu.MLE[1])} & \rinline{sprintf("%.2f",mu.JS[1])}\\
\rinline{names(maj.ct)[2]} & \rinline{sprintf("%.2f",mu[2])} & \rinline{sprintf("%.2f",mu.MLE[2])} & \rinline{sprintf("%.2f",mu.JS[2])}\\
\rinline{names(maj.ct)[3]} & \rinline{sprintf("%.2f",mu[3])} & \rinline{sprintf("%.2f",mu.MLE[3])} & \rinline{sprintf("%.2f",mu.JS[3])}\\
\rinline{names(maj.ct)[4]} & \rinline{sprintf("%.2f",mu[4])} & \rinline{sprintf("%.2f",mu.MLE[4])} & \rinline{sprintf("%.2f",mu.JS[4])}\\
\rinline{names(maj.ct)[5]} & \rinline{sprintf("%.2f",mu[5])} & \rinline{sprintf("%.2f",mu.MLE[5])} & \rinline{sprintf("%.2f",mu.JS[5])}\\
\rinline{names(maj.ct)[6]} & \rinline{sprintf("%.2f",mu[6])} & \rinline{sprintf("%.2f",mu.MLE[6])} & \rinline{sprintf("%.2f",mu.JS[6])}\\
\rinline{names(maj.ct)[7]} & \rinline{sprintf("%.2f",mu[7])} & \rinline{sprintf("%.2f",mu.MLE[7])} & \rinline{sprintf("%.2f",mu.JS[7])}\\
\rinline{names(maj.ct)[8]} & \rinline{sprintf("%.2f",mu[8])} & \rinline{sprintf("%.2f",mu.MLE[8])} & \rinline{sprintf("%.2f",mu.JS[8])}\\
\rinline{names(maj.ct)[9]} & \rinline{sprintf("%.2f",mu[9])} & \rinline{sprintf("%.2f",mu.MLE[9])} & \rinline{sprintf("%.2f",mu.JS[9])}\\
\rinline{names(maj.ct)[10]} & \rinline{sprintf("%.2f",mu[10])} & \rinline{sprintf("%.2f",mu.MLE[10])} & \rinline{sprintf("%.2f",mu.JS[10])}\\
\rinline{names(maj.ct)[11]} & \rinline{sprintf("%.2f",mu[11])} & \rinline{sprintf("%.2f",mu.MLE[11])} & \rinline{sprintf("%.2f",mu.JS[11])}\\
\rinline{names(maj.ct)[12]} & \rinline{sprintf("%.2f",mu[12])} & \rinline{sprintf("%.2f",mu.MLE[12])} & \rinline{sprintf("%.2f",mu.JS[12])}\\
\rinline{names(maj.ct)[13]} & \rinline{sprintf("%.2f",mu[13])} & \rinline{sprintf("%.2f",mu.MLE[13])} & \rinline{sprintf("%.2f",mu.JS[13])}\\
\rinline{names(maj.ct)[14]} & \rinline{sprintf("%.2f",mu[14])} & \rinline{sprintf("%.2f",mu.MLE[14])} & \rinline{sprintf("%.2f",mu.JS[14])}\\
\rinline{names(maj.ct)[15]} & \rinline{sprintf("%.2f",mu[15])} & \rinline{sprintf("%.2f",mu.MLE[15])} & \rinline{sprintf("%.2f",mu.JS[15])}\\
\rinline{names(maj.ct)[16]} & \rinline{sprintf("%.2f",mu[16])} & \rinline{sprintf("%.2f",mu.MLE[16])} & \rinline{sprintf("%.2f",mu.JS[16])}\\
\rinline{names(maj.ct)[17]} & \rinline{sprintf("%.2f",mu[17])} & \rinline{sprintf("%.2f",mu.MLE[17])} & \rinline{sprintf("%.2f",mu.JS[17])}\\ \hline
TSE &  & \rinline{sprintf("%.2f",TSE.MLE)} & \rinline{sprintf("%.2f",TSE.JS)} 
\end{tabular}
\end{center}
So, the ratio of the total square error is $TSE^{(MLE)} / TSE^{(JS)} =$ \rinline{sprintf("%.2f",TSE.MLE/TSE.JS)}.
And we can conclude that the Stein shrinkage in this application help moderately
reduce the total square error.

\end{document}
