\documentclass{article}\usepackage{graphicx, color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\definecolor{fgcolor}{rgb}{0.2, 0.2, 0.2}
\newcommand{\hlnumber}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlfunctioncall}[1]{\textcolor[rgb]{0.501960784313725,0,0.329411764705882}{\textbf{#1}}}%
\newcommand{\hlstring}[1]{\textcolor[rgb]{0.6,0.6,1}{#1}}%
\newcommand{\hlkeyword}[1]{\textcolor[rgb]{0,0,0}{\textbf{#1}}}%
\newcommand{\hlargument}[1]{\textcolor[rgb]{0.690196078431373,0.250980392156863,0.0196078431372549}{#1}}%
\newcommand{\hlcomment}[1]{\textcolor[rgb]{0.180392156862745,0.6,0.341176470588235}{#1}}%
\newcommand{\hlroxygencomment}[1]{\textcolor[rgb]{0.43921568627451,0.47843137254902,0.701960784313725}{#1}}%
\newcommand{\hlformalargs}[1]{\textcolor[rgb]{0.690196078431373,0.250980392156863,0.0196078431372549}{#1}}%
\newcommand{\hleqformalargs}[1]{\textcolor[rgb]{0.690196078431373,0.250980392156863,0.0196078431372549}{#1}}%
\newcommand{\hlassignement}[1]{\textcolor[rgb]{0,0,0}{\textbf{#1}}}%
\newcommand{\hlpackage}[1]{\textcolor[rgb]{0.588235294117647,0.709803921568627,0.145098039215686}{#1}}%
\newcommand{\hlslot}[1]{\textit{#1}}%
\newcommand{\hlsymbol}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlprompt}[1]{\textcolor[rgb]{0.2,0.2,0.2}{#1}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

\usepackage{color}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\today}
\chead{Ying Qiao SID:21412301}
\rhead{Stat215B Sp13: Assignment 7}
\lfoot{}
\cfoot{\thepage}
\rfoot{}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage{listings}
\usepackage{amssymb,amsmath}
\usepackage{bm}
\usepackage{hhline}

%% for inline R code: if the inline code is not correctly parsed, you will see a message
\newcommand{\rinline}[1]{SOMETHING WORNG WITH knitr}




\begin{document}
\section*{Regression, Prediction and Shrinkage}
In this assignment, we will demonstrate the comparison of prediction
accuarcy among several linear predictors, i.e. ordinary least square
(OLS) predictors, pre-shrunk predictors as in Copas(1983) and shrinkage
predictor by cross-validation.

\subsubsection*{Simulation Setup}
\hspace{12 pt} 
To demonstrate the required statements about prediction mean square
error (PMSE) of the three predictors. We choose the simulation
parameters as shown in the table below.




\begin{center}
%%\caption{Table 1}
\begin{tabular}{l|c|r} \hline
\texttt{Simulation Parameter} & \texttt{Symbol} & \texttt{Value}  \\ \hline
predictor dimension & $p$ & 50 \\ \hline
number of overall samples & $n$ & 100 \\
number of training samples & $n_{CS}$ & 80 \\
number of test samples & $n_{VS}$ & 20 \\ \hline
true intercept & $\alpha$ & 0.1 \\ 
true coefficients & $\bm{\beta}$ &  $\in$[0.1, 5]\\ \hline
noise-signal-ratio & $\delta^2$ & 0.01 \\
noise variance & $\sigma^2$ & 343.4 \\ \hline
simulation runs & $N$ & 100 \\ \hline
\end{tabular}
\end{center}

In order to show the strength of Copas(1983) method, we would like a
relatively large $p$ with a relatively small $n$. Also, the error
variance $\sigma^2$ should be comparable to $\bm{\beta^T}V\bm{\beta}$
with non-zero noise-signal-ratio $\delta^2$. So, as suggested by the
Breiman\&Friedman(1997) paper design, we set our simulation parameters
as above. The true linear model parameters, $\alpha, \bm{\beta}$ ,are not critical, so we
just made some simple choices. Moreover, for strong hypothesis testing
power, we have fairly large repetition runs, $N$, of simulations.


As for the data statistics, for each run, the predictor variables are
generated according to a normal distribution with zero mean and
covariance matrix $\bm{\Xi}$,
\begin{displaymath}
\begin{split}
\bm{x} &\sim  \mathcal{\bm{N}}(\bm{0}, \bm{\Xi})\\
\bm{\Xi} &= [V_{ij}]_{p\times p} , \hspace{12 pt} V_{ij} = r^{|i-j|},
\hspace{12 pt} r \sim \mathcal{U}[0,1]
\end{split}
\end{displaymath}
Also, the model matrix $\bm{X}$ is column centered for simplicity
(without loss of generality), and we have $\bm{V}=\frac{1}{n}\bm{X^TX}$. 
Meanwhile, for both \textit{construction sample}(CS) and
\textit{validation sample}(VS), the data sample vector
$\bm{y}$ are generated independently w.r.t each element,
\begin{displaymath}
\begin{split}
y &= \alpha + \bm{\beta^Tx} + \epsilon\\
\epsilon &\sim \mathcal{N}(0, \sigma^2)
\end{split}
\end{displaymath}
So, we can see that the randomness in my simulation comes from three
sources: first, the randomness between each run (rep. of dim=$N$); second, the
randomness in the data noise ($\epsilon$ of dim=$n$); and finally, the
randomness in the predictive factors ($x$ of dim=$p$).


Additionally, to verify the correctness of my implementation, we would
like to check if our estimates, $\hat{\sigma}^2$, are unbiased according to
a one-sample $t$-test with target $\mu_0=\sigma^2$.
\begin{displaymath}
\begin{split}
\hat{\sigma}^2 &= \frac{1}{\nu}\sum_{i=1}^{n_{CS}} (y_i - \hat{y}_i^{OLS})^2 \\
\nu &= n_{CS} - (p+1)
\end{split}
\end{displaymath}
And the two-sided $t$-test results in a p-value of
0.4777, 
which confirms that our estimates are unbiased.

Moreover, we also want to check if the $\bm{V}$ (as defined in
Copas(1983)) is close to the true covariance matrix $\bm{\Xi}$ in
terms of the Frobenius norm. 
\begin{displaymath}
\begin{split}
|\bm{V}||_F &= \sum_{i=1}^p \sum_{j=1}^p |V_{ij}|^2 \\
||\bm{\Xi}||_F &= \sum_{i=1}^p \sum_{j=1}^p |\Xi_{ij}|^2 \\
\end{split}
\end{displaymath}
We conduct a two-sample $t$-test on the $N$ samples of matrix norms
and get the p-value of 
0.8705,
which confirms that we can trust the $\bm{V}$ approximation.


Now, we are safe to proceed and check the hypothesis in the statements.


\subsubsection*{Statement 1: Copas vs. OLS}
\hspace{12 pt}  We want to evaluate


\hspace{12 pt} \textit{H0: the PMSE of Copas pre-shrunk predictor is
  the same as that of OLS} \newline
\vspace{2 pt}
\hspace{24 pt} \textit{HA: the PMSE of Copas pre-shrunk predictor is
  lower than that of OLS} \newline


We have the predictors with OLS parameter estimates, $\hat{\alpha},
\bm{\hat{\beta}}$, for CS data samples,
\begin{displaymath}
\begin{split}
\hat{y}^{OLS} &= \hat{\alpha} + \bm{\hat{\beta}^Tx} \\
\hat{y}^{Copas} &= \hat{\alpha} + \hat{K}\bm{\hat{\beta}^Tx},
\hspace{12 pt} \hat{K} = 1 - \frac{(p-2)\hat{\sigma}^2\nu}{n_{CS}(\nu+2)\bm{\hat{\beta}^TV\hat{\beta}}}
\end{split}
\end{displaymath}
and then conduct a one-sided two-sample $t$-test on the PMSE values obtained
from $N$ runs on the VS samples,
\begin{displaymath}
PMSE = \frac{1}{n_{VS}} \sum_{i=1}^{n_{VS}} (y_i - \hat{y}_i)^2
\end{displaymath}
the test gives a p-value of 
0.0043
which rejects the null hypothesis and shows that PMSE of Copas
pre-shrunk predictor is lower than that of OLS.


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[]


{\centering \includegraphics[width=\maxwidth]{figure/latex-ass7-1} 

}

\caption[PMSE Copas vs]{PMSE Copas vs. OLS\label{fig:ass7-1}}
\end{figure}


\end{knitrout}




\subsubsection*{Statement 2: Copas vs. Cross-validation}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[]


{\centering \includegraphics[width=\maxwidth]{figure/latex-ass7-2} 

}

\caption[PMSE Copas vs]{PMSE Copas vs. Cross-validation\label{fig:ass7-2}}
\end{figure}


\end{knitrout}


\hspace{12 pt}  We want to evaluate


\hspace{12 pt} \textit{H0: the PMSE of Copas pre-shrunk predictor is
  the same as that of shrinkage estimated from cross-validation} \newline
\vspace{2 pt}
\hspace{24 pt} \textit{HA: the PMSE of Copas pre-shrunk predictor is
  lower than that of shrinkage estimated from cross-validation} \newline


We have the predictors with OLS parameter estimates, $\hat{\alpha},
\bm{\hat{\beta}}$, for CS data samples,
\begin{displaymath}
\begin{split}
\hat{y}^{Copas} &= \hat{\alpha} + \hat{K}\bm{\hat{\beta}^Tx},
\hspace{12 pt} \hat{K} = 1 -\frac{(p-2)\hat{\sigma}^2\nu}{n_{CS}(\nu+2)\bm{\hat{\beta}^TV\hat{\beta}}}\\
\hat{y}^{CV} &= \hat{\alpha} + \tilde{K}\bm{\hat{\beta}^Tx},
\hspace{12 pt} \tilde{K} = \frac{1}{n_{CS}} \sum_{i=1}^{n_{CS}}K^{CV}_i\\
&\hspace{64 pt} K^{CV}_i = \frac{y_i}{\hat{y}_i},
\hspace{12 pt} \hat{y}_i = \hat{\alpha}^{/i} + (\bm{\hat{\beta}}^{/i})^T\bm{x}_i\\
\end{split}
\end{displaymath}
and then conduct a one-sided two-sample $t$-test on the PMSE values obtained
from $N$ runs on the VS samples, the test gives a p-value of 
\ensuremath{4.458\times 10^{-12}}
which rejects the null hypothesis and shows that PMSE of Copas
pre-shrunk predictor is lower than that of shrinkage estimated from cross-validation.
(Here, we set the $\delta^2$=\ensuremath{10^{-4}} for the simulation setup.)




\subsubsection*{Statement 3: Copas vs. Cross-validation | non-normal
  noise}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[]


{\centering \includegraphics[width=\maxwidth]{figure/latex-ass7-3} 

}

\caption[PMSE Copas vs]{PMSE Copas vs. Cross-validation given non-normal noise\label{fig:ass7-3}}
\end{figure}


\end{knitrout}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[]


{\centering \includegraphics[width=\maxwidth]{figure/latex-ass7-4} 

}

\caption[Histogram of eps]{Histogram of eps\label{fig:ass7-4}}
\end{figure}


\end{knitrout}

\hspace{12 pt}  We want to evaluate


\hspace{12 pt} \textit{H0: the PMSE of Copas pre-shrunk predictor is
  the same as that of shrinkage estimated from cross-validation under
  non-normal noise} \newline
\vspace{2 pt}
\hspace{24 pt} \textit{HA: the PMSE of Copas pre-shrunk predictor is
  larger than that of shrinkage estimated from cross-validation under
  non-normal noise} \newline


We have the same predictors as in statement 2, but here we do not have
a normal noise $\epsilon$, but a histogram shown in the Figure. Also,
we need a larger $\delta^2$ to strengthen the test, i.e. 0.04
Then we conduct a one-sided two-sample $t$-test on the PMSE values obtained
from $N$ runs on the VS samples, the test gives a p-value of 
\ensuremath{4.3421\times 10^{-6}}
which rejects the null hypothesis and shows that PMSE of Copas
pre-shrunk predictor is larger than that of shrinkage estimated from cross-validation.





\end{document}
